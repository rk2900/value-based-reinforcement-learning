% Copyright 2004 by Till Tantau <tantau@users.sourceforge.net>.
%
% In principle, this file can be redistributed and/or modified under
% the terms of the GNU Public License, version 2.
%
% However, this file is supposed to be a template to be modified
% for your own needs. For this reason, if you use this file as a
% template and not specifically distribute it as part of a another
% package/program, I grant the extra permission to freely copy and
% modify this file as you see fit and even to delete this copyright
% notice. 

\documentclass{beamer}

% There are many different themes available for Beamer. A comprehensive
% list with examples is given here:
% http://deic.uab.es/~iblanes/beamer_gallery/index_by_theme.html
% You can uncomment the themes below if you would like to use a different
% one:
%\usetheme{AnnArbor}
%\usetheme{Antibes}
%\usetheme{Bergen}
%\usetheme{Berkeley}
%\usetheme{Berlin}
%\usetheme{Boadilla}
%\usetheme{boxes}
\usetheme{CambridgeUS}
%\usetheme{Copenhagen}
%\usetheme{Darmstadt}
%\usetheme{default}
%\usetheme{Frankfurt}
%\usetheme{Goettingen}
%\usetheme{Hannover}
%\usetheme{Ilmenau}
%\usetheme{JuanLesPins}
%\usetheme{Luebeck}
% \usetheme{Madrid}
%\usetheme{Malmoe}
%\usetheme{Marburg}
%\usetheme{Montpellier}
%\usetheme{PaloAlto}
%\usetheme{Pittsburgh}
%\usetheme{Rochester}
%\usetheme{Singapore}
%\usetheme{Szeged}
%\usetheme{Warsaw}

\newcommand{\red}[1]{{\color{red} #1}}
\newcommand{\blue}[1]{{\color{blue} #1}}

\newcommand{\qa}{Q^{approx}}
\newcommand{\qt}{Q^{target}}
\newcommand{\ha}{\hat{a}}

\title{Value-based Reinforcement Learning}

% A subtitle is optional and this may be deleted
\subtitle{Some Discussions}

\author{Kan Ren}
% - Give the names in the same order as the appear in the paper.
% - Use the \inst{?} command only if the authors have different
%   affiliation.

\institute[SJTU] % (optional, but mostly needed)
{
%  \inst{1}%
  Apex Data and Knowledge Management Lab\\
  Shanghai Jiao Tong University
%  \and
%  \inst{2}%
%  Department of Theoretical Philosophy\\
%  University of Elsewhere
}
% - Use the \inst command only if there are several affiliations.
% - Keep it simple, no one is interested in your street address.

\date{Aug. 3 2017}
% - Either use conference name or its abbreviation.
% - Not really informative to the audience, more for people (including
%   yourself) who are reading the slides online

\subject{Theoretical Computer Science}
% This is only inserted into the PDF information catalog. Can be left
% out. 

% If you have a file called "university-logo-filename.xxx", where xxx
% is a graphic format that can be processed by latex or pdflatex,
% resp., then you can add a logo as follows:

\pgfdeclareimage[height=1cm]{university-logo}{apex_logo.png}
\logo{\pgfuseimage{university-logo}}

% Delete this, if you do not want the table of contents to pop up at
% the beginning of each subsection:
\AtBeginSubsection[]
{
  \begin{frame}<beamer>{Outline}
    \tableofcontents[currentsection,currentsubsection]
  \end{frame}
}

% Let's get started
\begin{document}

\begin{frame}
  \titlepage
\end{frame}

\begin{frame}{Outline}
  \tableofcontents
  % You might wish to add the option [pausesections]
\end{frame}

% Section and subsections will appear in the presentation overview
% and table of contents.
\section{Revision of Value-based RL}

\subsection{Dynamic Programming(omitted)}
\subsection{Monte-carlo Method(omitted)}
\subsection{TD: Sarsa and Q-learning}

\begin{frame}{Sarsa \& Q-learning}{Algorithm}
  \begin{figure}[t]
  	\includegraphics[width=0.7\columnwidth]{figures/sarsa-alg.jpg}
  \end{figure}

  \begin{figure}[t]
  	\includegraphics[width=0.7\columnwidth]{figures/q-learning-alg.jpg}
  \end{figure}
\end{frame}

\begin{frame}{Difference}
  \begin{itemize}
  	\item Exploration
  		\begin{itemize}
  			\item Sarsa: on-policy
  			\item Q-learning: off-policy
  		\end{itemize}
  	\item Update Rule
	  	\begin{itemize}
	  		\item Sarsa
	  		\begin{equation}\nonumber
	  		\begin{aligned}
	  		& Choose~ A'~ from~ S'~ using~policy~derived~from~Q~(e.g. \epsilon-greedy) \\
	  		& Q(S,A) \leftarrow Q(S,A) + \alpha[r + \gamma Q(S', A') - Q(S,A)]
	  		\end{aligned}
	  		\end{equation}
	  		\item Q-learning
	  		\begin{equation}\nonumber
	  		Q(S,A) \leftarrow Q(S,A) + \alpha[r + \gamma \max_a Q(S', a) - Q(S,A)]
	  		\end{equation}
	  	\end{itemize}
  \end{itemize}
\end{frame}




\section{Issues in Q-learning}

\subsection{Overestimation}
\begin{frame}{Overestimation}{Preliminaries}
	Recall that
	\begin{equation}
		Q(s,a) \longleftarrow r^a_s + \gamma~\red{\max}_{\ha} Q(s', \ha)
	\end{equation}
	Repeated application of this update equation eventually yields Q-values that give rise to \blue{a policy which maximizes the expected cumulative discounted reward}
	\footnote{C. J. C. H.Watkins, Learning from Delayed Rewards. PhD thesis, Kingâ€™s College, Cambridge, England, 1989.} in the look-up table case.
	
	The \red{$\max$} operation may cause some problems under the approximation scenario.
\end{frame}

\begin{frame}{Overestimation}
	Assume $Q^{approx}(\cdot)$ representing implicit target values $Q^{target}$, corrupted by a noise term $Y$ such that
	\begin{equation}\nonumber
		\qa(s',\ha) = \qt(s', \ha) + Y_{s'}^{\ha}
	\end{equation}
	
	\begin{equation}
	\begin{aligned}
		Z_s &\overset{def}{=} r_s^a + \gamma~ \max_{\ha} \qa(s', \ha) - \left( r_s^a + \gamma~  \max_{\ha} \qt(s', \ha)\right) \\
		& = \gamma~ \left( \max_{\ha} \qa(s', \ha) - \max_{\ha} \qt(s', \ha)\right)
	\end{aligned}
	\end{equation}
	
	The key observation is
	\begin{equation}
		E[Y_{s'}^{\ha}] = 0, ~ \forall \ha ~ \overset{often}{\Longrightarrow} E[Z_s] > 0 ~.
	\end{equation}
\end{frame}


\subsection{Double Q-learning}
\subsection{Averaged Q-learning}



\section{Convergence of Tabular TD}
\subsection{Sarsa}
\subsection{Q-learning}




\section{Deep Q-network}

\subsection{Nature DQN}
\subsection{Several Imrovements}


\end{document}


